# 创建SparkContext

## AppStatusStore

## SparkEnv

### SerializerManager

> 为spark各种组件配置序列化，压缩和加密的组件，包括自动选择在shuffle期间使用哪个Serializer

### BroadCastManager

+ TorrentBroadcast

  > Driver将序列化对象划分为小块，并将这些小块存储在Driver上的BlockManager中
  >
  > 在每个Executor上，Executor首先尝试从他们自己的BlockManager上获取对象，如果该对象不存在，就使用remote fetch从Driver或其他可用的Executor上获取数据块，每当获取到small chunks时，就将这些小块放到自己的BlockManager上，并可用于被其他的Executor获取
  >
  > 这可用防止Driver成为发送broadcast data多个副本【每个Executor一个】的瓶颈
  
+ Broadcast

  > 广播变量允许程序员在每台计算机上缓存一个只读变量，而不是将变量的副本与task一起被发送到每个节点。eg：可以使用广播变量以高效的方式为每个节点提供一个大型输入数据集的副本。Spark还尝试使用更有效的广播算法来分发广播变量来降低降低通信开销

### MapOutputTrackerMaster（conf,BroadCastManager,isLocal）

> Driver端的类用来跟踪一个stage的map output的location
>
> DAGScheduler使用这个类来注册map output status并且使用它来查找统计信息来执行位置感知的reduce task调度
>
> ShuffleMapStage使用这个类来跟踪可用或丢失的output来决定哪个任务需要被运行

### MapOutputTrackerWorker（conf）

> Executor端用来从Driver的MapOutputTrackerMaster中获取map output info的客户端。在local-mode下不会被使用到

### SortShuffleManager【默认使用sort shuffle manager】

> Sort-based shuffle
>
> 输入数据根据他们目标partition id进行排序，然后写入一个单独的map output file中。当map output data太大不能适应内存的话，已排序的map output的子集可以移除到磁盘，磁盘上的文件被合并为一个最终的输出文件。reducers从该文件中读取对应分区的数据

> Sort-based shuffle有两种不同的写入路径来生成其map output文件
>
> + Serialized sorting【需满足以下条件】
>   + shuffle dependence没有指定聚合或排序
>   + **shuffle serializer支持重新定位序列化值【目前KryoSerializer和Spark SQL‘s 自定义序列化器可以支持】**
>   + shuffle产生少于16777216个输出分区
>   + 优化项：
>     + 在Serialized sort模式下，传入的记录一旦传递到shuffle writer就会被序列化，并在排序期间以序列化的形式buffered。这种方式实现了几个优化：
>       + 排序操作的是序列化的二进制数据而不是java对象，这减少了内存和GC开销。这个优化需要数据的序列化器具有某些属性，以便在不需要反序列化的情况下重新排序序列化的记录【在SPARK-4550中，这个优化最初被提出和实现】
>       + 使用专门的高速缓存高效的排序器【ShuffleExternalSorter】，对压缩记录的指针数组和分区ids进行排序。通过在排序数组中对每条记录只使用8字节的空间，可以将更多的数组放入缓存中
>       + 溢出合并过程对属于同一分区的序列化记录块进行操作，并且在合并期间不需要反序列化记录
>       + 当spill压缩编解码器支持压缩数据的串联时，溢出合并只是将序列化和压缩的spill partitions串联起来来生成最终的输出分区。这允许使用高效的数据复制方法，如NIO的“transferTo”，并避免在合并期间分配解压或复制缓冲区的一些开销
> + Deserialized sorting【其他情况】

> ShuffleHandler：ShuffleManager将这个handle的信息传递给tasks
>
> + BypassMergeSortShuffleHandle【先判断是否满足使用这个的条件】
>   + 使用条件
>     + shuffleDependency中的partitioner的partitions < spark.shuffle.sort.bypassMergeThreshold
>     + 不需要map-side端聚合
>   
> + SerializedShuffleHandle【再判断这个】
>   + 使用条件
>     + shuffleDependency中的serializer支持重定位序列化对象【KryoSerializer支持】
>     + 不支持map-side端聚合
>     + shuffleDependency中的partitioner.numPartitions <= 一次shuffle中无序输出的最大分区数【16777216】，这是一种极端情况下的保护措施
>   
> + BaseShuffleHandle【其他情况下使用该Handle】：使用反序列化形式buffer map output
>
> + 源码
>
>   ```scala
>   override def registerShuffle[K, V, C](
>         shuffleId: Int,
>         numMaps: Int,
>         dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {
>       if (SortShuffleWriter.shouldBypassMergeSort(conf, dependency)) {
>         // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don't，need map-side aggregation, then write numPartitions files directly and just concatenate，them at the end. This avoids doing serialization and deserialization twice to merge，together the spilled files, which would happen with the normal code path. The downside is，having multiple files open at a time and thus more memory allocated to buffers.
>         new BypassMergeSortShuffleHandle[K, V](
>           shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
>       } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) {
>         new SerializedShuffleHandle[K, V](
>           shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
>       } else {
>         new BaseShuffleHandle(shuffleId, numMaps, dependency)
>       }
>   }
>   ```

### MemoryManager

> 每个JVM中存在一个MemoryManager
>
> execution memory：用于shuffle、join、sort、aggregation期间的计算
>
> storage memory：用于在集群中缓存和传播内部数据
>
> spark.memory.offheap.enabled：是否使用堆外内存

+ StaticMemoryManager

+ UnifiedMemoryManager

  ![image-20200613152943327](.\image\UnifiedMemoryManager.png)
  
  + 申请Storage memory
  
    ```scala
    override def acquireStorageMemory(
          blockId: BlockId,
          numBytes: Long,
          memoryMode: MemoryMode): Boolean = synchronized {
        assertInvariants()
        assert(numBytes >= 0)
        val (executionPool, storagePool, maxMemory) = memoryMode match {
          case MemoryMode.ON_HEAP => (
            onHeapExecutionMemoryPool,
            onHeapStorageMemoryPool,
            maxOnHeapStorageMemory)
          case MemoryMode.OFF_HEAP => (
            offHeapExecutionMemoryPool,
            offHeapStorageMemoryPool,
            maxOffHeapStorageMemory)
        }
        if (numBytes > maxMemory) {
          // 如果申请字节大小 > 最大StorageMemory，fail-fast
          logInfo(s"Will not store $blockId as the required space ($numBytes bytes) exceeds our " +
            s"memory limit ($maxMemory bytes)")
          return false
        }
        if (numBytes > storagePool.memoryFree) {
          // 如果StorageMemory中的空闲内存 < 申请字节大小，那么从Execution Memory中借【memoryBorrowedFromExecution】放入到StorageMemory中
          val memoryBorrowedFromExecution = Math.min(executionPool.memoryFree,
            numBytes - storagePool.memoryFree)
          executionPool.decrementPoolSize(memoryBorrowedFromExecution) // ExecutionMemory总量下降
          storagePool.incrementPoolSize(memoryBorrowedFromExecution) // StorageMemory总量上升
        }
        storagePool.acquireMemory(blockId, numBytes)
    }
    def acquireMemory(
          blockId: BlockId,
          numBytesToAcquire: Long,
          numBytesToFree: Long): Boolean = lock.synchronized {
        assert(numBytesToAcquire >= 0)
        assert(numBytesToFree >= 0)
        assert(memoryUsed <= poolSize)
        if (numBytesToFree > 0) {
          // 如果StorageMemory中可用内存仍然小于申请的内存大小numBytesToAcquire，那么就从StorageMemory中踢出去一部分block
          memoryStore.evictBlocksToFreeSpace(Some(blockId), numBytesToFree, memoryMode)
        }
        // NOTE: If the memory store evicts blocks, then those evictions will synchronously call
        // back into this StorageMemoryPool in order to free memory. Therefore, these variables
        // should have been updated.
        val enoughMemory = numBytesToAcquire <= memoryFree
        if (enoughMemory) {
          _memoryUsed += numBytesToAcquire
        }
        enoughMemory
    }
    ```

### BlockTransferService【NettyBlockTransferService】

> NettyBlockTransferService：使用Netty获取获取一组blocks

### BlockManagerMaster

> 线程安全，位与master节点，跟踪所有slaves' block manager的状态

+ 移除RDD流程
  + 首先移除指定RDD的元数据，然后异步从slaves中移除相应的blocks
  + 根据blockLocations定位到哪些BlockId.asRDDId.rddId = RDDId
  + 定位跟踪这些Block的BlockManagers
  + 通知这些BlockManager移除这些Blocks

### BlockManager

> 在每个节点【driver和executor】上运行的管理器，提供put和取回本地或远程的blocks并放入不同的存储【内存、磁盘、off-heap】中的接口
>
> 调用initialize方法之后才可用

+ DiskManager
  + 创建并维护逻辑block和物理block的磁盘位置的映射关系。一个block对应一个文件名为BlockId的文件
  + block文件被hash分布在【spark.local.dir】指定的目录下
  
+ MemStore
  
  + 将block存储在memory中，可用是反序列化的Java对象数组或序列化的ByteBuffers
  
+ DIskStore
  
  + 将BlockManager的blocks存储在磁盘上
  
+ BlockReplicationPolicy

  + BasicBlockReplicationPolicy

    + 与HDFS相似，在机架内找到一个BlockManager，随机选择一个外部的和剩余的BlockManager，直到达到副本复制数量

      ```scala
      override def prioritize(
            blockManagerId: BlockManagerId,
            peers: Seq[BlockManagerId],
            peersReplicatedTo: mutable.HashSet[BlockManagerId],
            blockId: BlockId,
            numReplicas: Int): List[BlockManagerId] = {
      
          logDebug(s"Input peers : $peers")
          logDebug(s"BlockManagerId : $blockManagerId")
      
          val random = new Random(blockId.hashCode)
      
          // 如果不包含网络拓扑信息，或者副本数==0，那么就随机获取numReplicas个候选BlockManager
          if (blockManagerId.topologyInfo.isEmpty || numReplicas == 0) {
            // no topology info for the block. The best we can do is randomly choose peers
            BlockReplicationUtils.getRandomSample(peers, numReplicas, random)
          } else {
            // 判断当前已选择的候选BlockManagers是否包含机架内和机架外的节点
            val doneWithinRack = peersReplicatedTo.exists(_.topologyInfo == blockManagerId.topologyInfo)
            val doneOutsideRack = peersReplicatedTo.exists { p =>
              p.topologyInfo.isDefined && p.topologyInfo != blockManagerId.topologyInfo
            }
      
            if (doneOutsideRack && doneWithinRack) {
              // 如果即包含机架内和机架外的节点，那么随机选择numReplicas个候选BlockManagers
              BlockReplicationUtils.getRandomSample(peers, numReplicas, random)
            } else {
              // 否则，将所有的候选节点划分为机架内和机架外的节点
              val (inRackPeers, outOfRackPeers) = peers
                  .filter(_.host != blockManagerId.host)
                  .partition(_.topologyInfo == blockManagerId.topologyInfo)
      		
              val peerWithinRack = if (doneWithinRack) {
                // 如果已选择的节点中已经包含机架内的节点了，那么返回空
                Seq.empty
              } else {
      		  // 同样返回空
                if (inRackPeers.isEmpty) {
                  Seq.empty
                } else {
      		  // 否则从机架内的节点中随机选择一个
                  Seq(inRackPeers(random.nextInt(inRackPeers.size)))
                }
              }
      		// 如果已选择的节点中已经包含机架外的节点了或者分配的节点数已经满足副本个数了，那么返回空
              val peerOutsideRack = if (doneOutsideRack || numReplicas - peerWithinRack.size <= 0) {
                Seq.empty
              } else {
      		  // 同样返回空
                if (outOfRackPeers.isEmpty) {
                  Seq.empty
                } else {
      		  // 否则从机架外的节点中随机选择一个
                  Seq(outOfRackPeers(random.nextInt(outOfRackPeers.size)))
                }
              }
      		
              val priorityPeers = peerWithinRack ++ peerOutsideRack
      		// 剩下的未分配的副本
              val numRemainingPeers = numReplicas - priorityPeers.size
      		// 从剩下的未分配的节点上随机选择相应数量的节点
              val remainingPeers = if (numRemainingPeers > 0) {
                val rPeers = peers.filter(p => !priorityPeers.contains(p))
                BlockReplicationUtils.getRandomSample(rPeers, numRemainingPeers, random)
              } else {
                Seq.empty
              }
      
              (priorityPeers ++ remainingPeers).toList
            }
      
          }
        }
      ```

  + RandomBlockReplicationPolicyy

    ```scala
    // 随机采样，尽可能将Block放到不同的节点上
    override def prioritize(
          blockManagerId: BlockManagerId,
          peers: Seq[BlockManagerId],
          peersReplicatedTo: mutable.HashSet[BlockManagerId],
          blockId: BlockId,
          numReplicas: Int): List[BlockManagerId] = {
        val random = new Random(blockId.hashCode)
        logDebug(s"Input peers : ${peers.mkString(", ")}")
        val prioritizedPeers = if (peers.size > numReplicas) {
          BlockReplicationUtils.getRandomSample(peers, numReplicas, random)
        } else {
          if (peers.size < numReplicas) {
            logWarning(s"Expecting ${numReplicas} replicas with only ${peers.size} peer/s.")
          }
          random.shuffle(peers).toList
        }
        logDebug(s"Prioritized peers : ${prioritizedPeers.mkString(", ")}")
        prioritizedPeers
      }
    ```

### MetricsSystem

> 在task scheduler返回一个app id之后再启动这个指标系统

> 使用该系统的一些角色：Master、Worker、Executor、Client Driver

### OutputCommitCoordinator

> 决定task是否可用将输出提交给HDFS，使用“first committer wins”策略
>
> 在Driver端和Executor端都会初始化，在Executor端会配置一个到Driver端OutputCommitCoordinatorEndpoint的引用,因此提交output的请求将被转发到Driver上的OutputCommitCoordinator上

## SparkStatusTracker

> 用于监控Job和Stage进度状态的底层reporting API
>
> 提供非常弱的一致性语义而且为了限制内存的使用，值提供最近的job和stages信息。只提供最近【spark.ui.retainedStages】个stages和【spark.ui.retainedJobs】个job的信息

## SparkUI

## HeartbeatReceiver

> 存在于Driver端，用于接收Executors的心跳

## TaskScheduler

> 通过SchedulerBackend为多种类型的Cluster调度任务
>
> 处理通用的逻辑，如确定job之间的调度顺序，唤醒以启动推测执行的Tasks
>
> 客户端应先调用initialize方法和start方法，然后再通过submitTasks方法提交task sets

+ ScheduleMode：默认FIFO
  + FIFO【默认】
  + FAIR
+ SchedulingAlgorithm
  + FIFOSchedulingAlgorithm
  + FairSchedulingAlgorithm
+ BarrierCoordinator
  + 处理所有来自BarrierTaskContext的全局同步请求。每个全局同步请求由BarrierTaskContext.barrier()产生，由stageId+stageAtemptId+barrierEpoch标识。当接收到一组barrier()方法产生的request时需要回复所有的这些全局同步请求。如果在一定的时间内【可配置】没有收到足够的全局同步请求，则使所有请求失败，并返回一个带有超时消息的异常

## DAGScheduler

> 面向Stage调度的高层次调度

> + 为每个job构建基于stages的DAG，跟踪哪些RDDs和Stages output已经被物化【已被输出到磁盘】，寻找运行job的最小的调度计划。
> + 将stages作为Tasksets提交给底层的可以将这些task运行在集群中的TaskScheuler实例上。一个Taskset包含完全独立的任务，这些任务可以基于集群中已有的数据（先前stage的map output文件）立即运行，如果这些数据不可用，则可能失败
> + stages是根据在RDD依赖图中的shuffle边界划分和创建的。
>   + narrow dependence
>   + shuffle dependence
> + 根据当前缓存状态决定运行每个task的首选位置，并将这些节点位置信息传递给TaskScheduler。此外，他还处理因为shuffle output file丢失而造成的任务失败，在这种情况下，old stages需要被重新提交。而在stage内且并不是由于shuffle file loss导致的任务失败由TaskScheduler处理，在这种情况下，在取消整个stage之前会对失败的task进行次数不多的重试

> 关键概念
>
> + Job是提交给scheduler的顶层工作单元。当用户调用一个action，例如count,此时一个Job将通过submitJob方法被提交。每个Job可能需要多个stage的执行来构建中间数据
>
> + Stages是一组计算Job中间结果的tasks，每个task同一个RDD的不同分区上执行相同的逻辑计算。Stages在shuffle边界处隔开，这里会引入一个barrier【在这里我们必须等待前一个stage完成才能获取前一个stage的output】
>   + ResultStage
>
>     + 执行action算子会触发JobSubmit，之后会根据finalRDD、jobId等相关信息创建ResultStage
>
>       ```scala
>       finalStage:ResultStage = createResultStage(finalRDD,func,partitions,jobId,callSite)
>       ```
>
>       ```scala
>       private def createResultStage(
>             rdd: RDD[_],
>             func: (TaskContext, Iterator[_]) => _,
>             partitions: Array[Int],
>             jobId: Int,
>             callSite: CallSite): ResultStage = {
>           // 不支持使用动态资源分配的方式运行barrier stage，如果返回true就会fail-fast
>           checkBarrierStageWithDynamicAllocation(rdd)
>           // 检查barrier stage申请的slots是否>集群中总的可用的slot数量
>           // 也就是【所有Executor上的cores总和 / scheduler.CPUS_PER_TASK】
>           // CPUS_PER_TASK = conf.getInt("spark.task.cpus", 1)
>           checkBarrierStageWithNumSlots(rdd)
>           checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)
>           val parents = getOrCreateParentStages(rdd, jobId)
>           val id = nextStageId.getAndIncrement()
>           val stage = new ResultStage(id, rdd, func, partitions, parents, jobId, callSite)
>           stageIdToStage(id) = stage
>           updateJobIdStageIdMaps(jobId, stage)
>           stage
>       ```
>   + ShuffleMapStage
>     
>     + 为shuffle写入map output files
>     
>     + 根据ShuffleDependency创建ShuffleMapStage
>     
>       ```scala
>       def createShuffleMapStage(shuffleDep: ShuffleDependency[_, _, _], jobId: Int): ShuffleMapStage = {
>           // shuffle map端rdd
>           val rdd = shuffleDep.rdd
>           // 校验
>           checkBarrierStageWithDynamicAllocation(rdd)
>           checkBarrierStageWithNumSlots(rdd)
>           checkBarrierStageWithRDDChainPattern(rdd, rdd.getNumPartitions)
>           // map端rdd分区个数
>           val numTasks = rdd.partitions.length
>           // 获取父级ShuffleMapStage
>           val parents = getOrCreateParentStages(rdd, jobId)
>           val id = nextStageId.getAndIncrement()
>           val stage = new ShuffleMapStage(
>             id, rdd, numTasks, parents, jobId, rdd.creationSite, shuffleDep, mapOutputTracker)
>           // 维护相关stage映射关系
>           stageIdToStage(id) = stage
>           shuffleIdToMapStage(shuffleDep.shuffleId) = stage
>           updateJobIdStageIdMaps(jobId, stage)
>           if (!mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) {
>             // Kind of ugly: need to register RDDs with the cache and map output tracker here
>             // since we can't do it in the RDD constructor because # of partitions is unknown
>             logInfo("Registering RDD " + rdd.id + " (" + rdd.getCreationSite + ")")
>             mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length)
>           }
>           stage
>         }
>       ```
>     
>   
> + Task是单个工作单元，每个工作单元被发送到一个节点
>
> + DAGScheduler找出哪些Rdds被缓存以避免重新计算他们，并且同样记住哪些ShuffleMapStage已经生成了输出文件，以避免重新执行shuffle阶段的map端任务
>
> + DAGScheduler还根据RDDs的首选位置或缓存或shuffle data的位置来决定在哪里执行stage中的每个task
>
> + 当依赖于相关数据结构的Job完成时，这些数据结构将被清除来防止长时间运行的Job程序中出现内存泄漏

> **提交Stage**：首先递归提交missing parents
>
> ```scala
> private def submitStage(stage: Stage) {
>     val jobId = activeJobForStage(stage)
>     if (jobId.isDefined) {
>       logDebug("submitStage(" + stage + ")")
>       if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {
>         val missing = getMissingParentStages(stage).sortBy(_.id)
>         logDebug("missing: " + missing)
>         if (missing.isEmpty) {
>           logInfo("Submitting " + stage + " (" + stage.rdd + "), which has no missing parents")
>           submitMissingTasks(stage, jobId.get) // 源码见下文
>         } else {
>           for (parent <- missing) {
>             submitStage(parent)
>           }
>           waitingStages += stage
>         }
>       }
>     } else {
>       abortStage(stage, "No active job for stage " + stage.id, None)
>     }
>   }
> ```
>
> **提交missing tasks**
>
> ```scala
> private def submitMissingTasks(stage: Stage, jobId: Int) {
>     logDebug("submitMissingTasks(" + stage + ")")
> 
>     // First figure out the indexes of partition ids to compute.
>     val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()
> 
>     // Use the scheduling pool, job group, description, etc. from an ActiveJob associated
>     // with this Stage
>     val properties = jobIdToActiveJob(jobId).properties
> 
>     runningStages += stage
>     // SparkListenerStageSubmitted should be posted before testing whether tasks are
>     // serializable. If tasks are not serializable, a SparkListenerStageCompleted event
>     // will be posted, which should always come after a corresponding SparkListenerStageSubmitted
>     // event.
>     stage match {
>       case s: ShuffleMapStage =>
>         outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)
>       case s: ResultStage =>
>         outputCommitCoordinator.stageStart(
>           stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)
>     }
>     val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {
>       stage match {
>         case s: ShuffleMapStage =>
>           partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap
>         case s: ResultStage =>
>           partitionsToCompute.map { id =>
>             val p = s.partitions(id)
>             (id, getPreferredLocs(stage.rdd, p))
>           }.toMap
>       }
>     } catch {
>       case NonFatal(e) =>
>         stage.makeNewStageAttempt(partitionsToCompute.size)
>         listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
>         abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
>         runningStages -= stage
>         return
>     }
> 
>     stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)
> 
>     // If there are tasks to execute, record the submission time of the stage. Otherwise,
>     // post the even without the submission time, which indicates that this stage was
>     // skipped.
>     if (partitionsToCompute.nonEmpty) {
>       stage.latestInfo.submissionTime = Some(clock.getTimeMillis())
>     }
>     listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))
> 
>     // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.
>     // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast
>     // the serialized copy of the RDD and for each task we will deserialize it, which means each
>     // task gets a different copy of the RDD. This provides stronger isolation between tasks that
>     // might modify state of objects referenced in their closures. This is necessary in Hadoop
>     // where the JobConf/Configuration object is not thread-safe.
>     var taskBinary: Broadcast[Array[Byte]] = null
>     var partitions: Array[Partition] = null
>     try {
>       // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).
>       // For ResultTask, serialize and broadcast (rdd, func).
>       var taskBinaryBytes: Array[Byte] = null
>       // taskBinaryBytes and partitions are both effected by the checkpoint status. We need
>       // this synchronization in case another concurrent job is checkpointing this RDD, so we get a
>       // consistent view of both variables.
>       RDDCheckpointData.synchronized {
>         taskBinaryBytes = stage match {
>           case stage: ShuffleMapStage =>
>             JavaUtils.bufferToArray(
>               closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))
>           case stage: ResultStage =>
>             JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))
>         }
> 
>         partitions = stage.rdd.partitions
>       }
> 
>       taskBinary = sc.broadcast(taskBinaryBytes)
>     } catch {
>       // In the case of a failure during serialization, abort the stage.
>       case e: NotSerializableException =>
>         abortStage(stage, "Task not serializable: " + e.toString, Some(e))
>         runningStages -= stage
> 
>         // Abort execution
>         return
>       case e: Throwable =>
>         abortStage(stage, s"Task serialization failed: $e\n${Utils.exceptionString(e)}", Some(e))
>         runningStages -= stage
> 
>         // Abort execution
>         return
>     }
> 
>     val tasks: Seq[Task[_]] = try {
>       val serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()
>       stage match {
>         case stage: ShuffleMapStage =>
>           stage.pendingPartitions.clear()
>           partitionsToCompute.map { id =>
>             val locs = taskIdToLocations(id)
>             val part = partitions(id)
>             stage.pendingPartitions += id
>             new ShuffleMapTask(stage.id, stage.latestInfo.attemptNumber,
>               taskBinary, part, locs, properties, serializedTaskMetrics, Option(jobId),
>               Option(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())
>           }
> 
>         case stage: ResultStage =>
>           partitionsToCompute.map { id =>
>             val p: Int = stage.partitions(id)
>             val part = partitions(p)
>             val locs = taskIdToLocations(id)
>             new ResultTask(stage.id, stage.latestInfo.attemptNumber,
>               taskBinary, part, locs, id, properties, serializedTaskMetrics,
>               Option(jobId), Option(sc.applicationId), sc.applicationAttemptId,
>               stage.rdd.isBarrier())
>           }
>       }
>     } catch {
>       case NonFatal(e) =>
>         abortStage(stage, s"Task creation failed: $e\n${Utils.exceptionString(e)}", Some(e))
>         runningStages -= stage
>         return
>     }
> 
>     if (tasks.size > 0) {
>       logInfo(s"Submitting ${tasks.size} missing tasks from $stage (${stage.rdd}) (first 15 " +
>         s"tasks are for partitions ${tasks.take(15).map(_.partitionId)})")
>       taskScheduler.submitTasks(new TaskSet(
>         tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties))
>     } else {
>       // Because we posted SparkListenerStageSubmitted earlier, we should mark
>       // the stage as completed here in case there are no tasks to run
>       markStageAsFinished(stage, None)
> 
>       stage match {
>         case stage: ShuffleMapStage =>
>           logDebug(s"Stage ${stage} is actually done; " +
>               s"(available: ${stage.isAvailable}," +
>               s"available outputs: ${stage.numAvailableOutputs}," +
>               s"partitions: ${stage.numPartitions})")
>           markMapStageJobsAsFinished(stage)
>         case stage : ResultStage =>
>           logDebug(s"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})")
>       }
>       submitWaitingChildStages(stage)
>     }
>   }
> ```

## Dependency

+ NarrowDependency：子RDD的每个分区依赖于父RDD的少量分区，这种依赖运行pipeline execution

  + OneToOneDependency

    + 子RDD分区与父RDD每个分区一一对应

  + PruneDependency

    ![image-20200614112844513](E:\workspace\bigdata\笔记\Spark\image\PruneDependency.png)

  + RangeDependency

    + 父RDD的某个分区范围和子RDD某个分区范围一一对应

      ![image-20200614110510178](.\image\RangeDependence.png)

+ ShuffleDependence：代表对Shuffle Stage output的一个依赖关系

  + getShuffleDependencies

    ```scala
    // 只获取指定rdd的直接父ShuffleDependency
    private[scheduler] def getShuffleDependencies(
          rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] = {
        val parents = new HashSet[ShuffleDependency[_, _, _]]
        val visited = new HashSet[RDD[_]]
        val waitingForVisit = new ArrayStack[RDD[_]]
        waitingForVisit.push(rdd)
        while (waitingForVisit.nonEmpty) {
          val toVisit = waitingForVisit.pop()
          if (!visited(toVisit)) {
            visited += toVisit
            toVisit.dependencies.foreach {
              case shuffleDep: ShuffleDependency[_, _, _] =>
                parents += shuffleDep
              case dependency =>
                waitingForVisit.push(dependency.rdd)
            }
          }
        }
        parents
      }
    ```

    


## Partitioner

+ 获取默认分区器

  ```scala
  def defaultPartitioner(rdd: RDD[_], others: RDD[_]*): Partitioner = {
      val rdds = (Seq(rdd) ++ others)
      // 存在分区器的rdds
      val hasPartitioner = rdds.filter(_.partitioner.exists(_.numPartitions > 0))
      // 存在分区器的rdds中的分区数量最大的maxRDD
    val hasMaxPartitioner: Option[RDD[_]] = if (hasPartitioner.nonEmpty) {
        Some(hasPartitioner.maxBy(_.partitions.length))
      } else {
        None
      }
      // 获取默认分区数
      val defaultNumPartitions = if (rdd.context.conf.contains("spark.default.parallelism")) {
        // 如果配置了该属性，那么就取该属性  
        rdd.context.defaultParallelism
      } else {
        // 否则，取所有rdds中最大的分区数量作为默认分区数
        rdds.map(_.partitions.length).max
      }
      // hasMaxPartitioner不为空，且hasMaxPartitioner的RDD的分区数量与rdds中最大的分区数不超过1个数量级，以10为底 || defaultNumPartitions < hasMaxPartitioner的RDD的分区数量
      if (hasMaxPartitioner.nonEmpty && (isEligiblePartitioner(hasMaxPartitioner.get, rdds) ||
          defaultNumPartitions < hasMaxPartitioner.get.getNumPartitions)) {
        // 使用已存在的分区器
        hasMaxPartitioner.get.partitioner.get
      } else {
        // 使用HashPartitioner
        new HashPartitioner(defaultNumPartitions)
      }
    }
  ```
  

## JobWaiter

> 等待DAGScheduler job完成的对象，当任务完成后，它会将其结果传递给给定的hadnler function